#!/bin/bash

version=0.1.0

__dirname=$(dirname "$(readlink -f "$0")")
. "$(readlink -f "$__dirname/functions.sh")"

if [ $# -lt 1 ]; then
 print_help
 exit 1
fi

# parse valid options
parse_options $@

if $verbose; then
  echo $CRAWLER_DEPTH
fi

# content = $target !== '' ? $(wget -qO - $target) || ''
[[ -n $target ]] && content=$(wget -qO - $target) || content=''

if [[ -z $content ]]; then
  echo "The site $1 content could not be reached"
  exit 1
fi

keep_reading=1
i=0

# remove the head before procesing
content=${content##*<body}

while [[ $keep_reading -eq 1 ]]; do
  if [[ $content == *href=* ]]; then
    # 1. Remove everything before the first href
    remove=${content#*href=[\"|\']}
    # 2. get the first link
    link=${remove%%[\"|\']*}
    links[$i]=$link
    (( i++ ))
    # 3. search in the rest of the string
    content=${remove#*[\"|\']\>}
  else
    # stop when there is no more 'href' in the given string
    keep_reading=0
  fi
done
unique_links=($(echo "${links[@]}" | tr ' ' '\n' | sort -u | tr '\n' ' '))
final_links=(filter_protocol "${unique_links[@]}")

result[$target]=("${final_links[@]}")

if [[ $CRAWLER_DEPTH -gt 0 ]]; then
  (( CRAWLER_DEPTH-- ))
  for flink in "${!final_links[@]}"; do
    ./$0 -d $CRAWLER_DEPTH $flink
  done
else
  export result
  export CRAWLER_DEPTH
  print_result
fi
